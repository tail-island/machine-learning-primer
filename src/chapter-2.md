# 機械学習の手法の選択

本稿では、これまで線形回帰、決定木、ランダム・フォレスト、K-meansといういくつかの手法を使用してきました。実は機械学習にはこれ以外にもたくさんの手法があって、それを使い分けなければならない……のは大変すぎるので、私が手法選択に使用しているルールを疑似コードで示しましょう。

~~~python
if (テーブル・データではない and
    大量のデータを用意できる and
    (そこそこ高性能なGPUを使える or
     潤沢なクラウド予算がある)):
    return 深層学習
else
    return 勾配ブースティング
~~~

……ごめんなさい。実は私は普段は線形回帰とかやってません。なんでかというと、私は統計の人じゃなくて機械学習の人だからです。

## 統計と機械学習

統計と機械学習って同じ道具を使うことも多くて似ているのですけど、目的が異なります。統計は「説明」を目的とし、機械学習は「予測」を目的とします。気温とアイスクリームの売り上げには関係があるのか、関係があるのであればどのような関係なのかを考えるのが統計で、どんな関係なのかは重視しないで予測の精度を高めることにひたすら注力するのが機械学習なんです。

統計をしたいのであれば、データの関係を明らかにしたいですから、モデルはできるだけ単純な方が良い。シンプルなモデルの方が関係が分かりやすいですもんね。だから、データに合致する範囲で、できるだけシンプルなモデルを構築します。だから、まずは線形回帰で関係があるかを調べようとなります。

機械学習では予測精度をできるだけ高めたいので、複雑なモデルであっても、モデルの中身がブラックボックスでデータ間の関係が理解不能であってもお構いなし。だから、できるだけ高い予測精度を期待できるモデルを選びます。よって、線形回帰みたいな精度が低そうなモデルは選択肢に上がらないわけ。

で、私は機械学習の人なので、前に書いたように、現在の技術で最高の精度を誇る深層学習（deep learning）と勾配ブースティング（gradient boosting）しか使わないんですな。

## 深層学習の弱点

いやいや精度ならば深層学習一択という反論が聞こえてきそうですけど、深層学習って弱点も多いと思うんですよ。

深層学習は脳の構造を模した手法とかよく言われますけど、今どきの深層学習モデルは脳のどこかを模しているわけではありません。独自に進化して、脳とはあまり関係ないものになっているんじゃないかと。

ぶっちゃけて言えば、私は深層学習とは行列演算を大量に積み重ねたもので、その行列の各要素をパラメーターとする、異常に大量のパラメーターを使える機械学習の手法と認識しています。で、パラメーターが大量にあったら調整が大変に思えるのですけど、それは逆誤差伝播法（backprobagation）という適用可能な範囲は狭いけど効率が良い調整方式で対応しちゃう。あと、大量のパラメーターを使って計算するのは時間がかかりそうに思えるけど、それは行列の特性をイイ感じに活用した並列処理で対応しちゃう。

このように弱点が見えない深層学習ですけど、深層学習と言われて思い浮かべる以下の図のモデルは全結合（dense）と呼ばれる層を重ねたもので、実は、そんなに精度が高くありません。

![]()

### すべては畳み込みから

その深層学習の精度が大きく向上したのは、畳み込み（convolution）という手法が発明されてから。で、この畳み込みってのは、画像処理でいうところのフィルタリングそのものなんです。

画像においては、あるピクセルだけじゃなくて、そのピクセルの上下左右やそのさらに外側のピクセルとの関係が重要ですよね？　白い中に黒いピクセルが縦に並んでるから縦線と認識できるわけで。このような周りとの関係を汎用的な手法で抽出するのがフィルタリングです。

フィルタリング処理の具定例を示しましょう。こんな感じ。

~~~python
import cv2
import matplotlib.pyplot as plot
import numpy as np


def filter(image, kernel):
    result = np.zeros((np.shape(image)[0] - 2, np.shape(image)[1] - 2))

    for y in range(np.shape(result)[0]):
        for x in range(np.shape(result)[1]):
            # 画像の該当部分とカーネルの内積を求めて新たな画像を作成します。
            result[y, x] = np.dot(image[y: y + 3, x: x + 3].flatten(), kernel.flatten())

    result[result <   0] =   0  # noqa: E222
    result[result > 255] = 255

    return result


image = cv2.cvtColor(cv2.imread('4.2.07.tiff'), cv2.COLOR_RGB2GRAY)

plot.imshow(image)
plot.show()

plot.imshow(filter(image, np.array([[0, 2, 0], [2, -8, 2], [0, 2, 0]])))
plot.show()
~~~

ある点の周囲の画像と行列（コード中の`kernel`）をベクトルに変換（`flatten()`）して内積（`np.dot()`）して、新たな画像を作るわけですね。

で、本稿は文系のための文書ですから、内積について補足しましょう。でも数式は嫌なので、具体例でやります。題材は、転職で。転職先を、給料と仕事の楽しさで表現してみます。

![]()

で、あれもこれも欲しいというのは許されないでしょうから、転職先の評価軸は、距離が1のベクトルで表現するとしましょう。上の図の、左下から円周上のどこかに向かうベクトルが、転職先の評価軸となるわけですね。

で、「高い給料をもらえるなら非合法な仕事でもオッケー」とか「仕事が楽しければ霞を食べて生きていける」という特殊な場合はそれぞれX軸とY軸の値を見るだけで良いのですけど、普通は、給料6割で仕事の楽しさ4割とかで評価したいですよね？　そんな時は、内積が役に立ちます。

![]()

上の図のように、内積は、あるベクトルの視点で、他のベクトルがどの程度の量になるのかを表現しています。この評価軸のベクトル上での、各転職先の大きさはどのくらいなのかがこれで一目瞭然で転職がはかどっちゃうこと請け合い。しかも、内積ってのは2次元の画像だけじゃなくて、3次元でも4次元でも、1,024次元とかでも成り立つんですよ。だから、先ほどのコードでの`kernel`のような、9次元のベクトルでも使えるんです。

でね、先ほどのプログラムの`kernel`の値は、輪郭の場合に大きな値になるような行列になっています。輪郭抽出なんて機械学習に関係なさそうと思ったかもしれませんけど、もし、こんな感じに丸くなっているとか、こんな感じに尖っているとか、こんな感じに交差しているとかを表現するベクトルを作れば、しかもそれが大量にあるのであれば、たとえば文字認識とかが高い精度でできると思いませんか？

深層学習は機械学習なので、どのような`kernel`を使うと文字認識に有効なのかとか、犬と猫を区別するにはどのような`kernel`があればできるのかとかはコンピューターが調整してくれます。手で行列を作る必要はなくて、だからとても楽ちん。しかも精度が高い！

と、これが畳み込みで、この畳み込みのおかげで深層学習での画像認識の精度は大幅に向上したんですよ。

### 今は、アテンションで


